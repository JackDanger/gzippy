# =============================================================================
# Unified Benchmarks: Compression + Decompression in one efficient workflow
# =============================================================================
# Architecture:
#   Stage 1: Build all tools ONCE
#   Stage 2: Prepare test data ONCE per data_type
#   Stage 3: Benchmark matrix (level Ã— threads Ã— data_type)
#   Stage 4: Guards - apply threshold assertions
#   Stage 5: Summary - aggregate and post to PR
#
# This replaces: compression.yml, decompression.yml, performance-regression.yml
# =============================================================================
name: Benchmarks

on:
  push:
    branches: ["*"]
  pull_request:
    branches: ["*"]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  # ============================================================================
  # STAGE 1: Build all tools once, share via artifacts
  # ============================================================================
  build:
    name: Build Tools
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake nasm zlib1g-dev

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Build all tools
        run: |
          # Build gzippy
          cargo build --release
          
          # Build pigz
          make -C pigz
          
          # Build igzip (static)
          cd isa-l
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF
          make -j$(nproc) igzip || echo "igzip build failed"
          cd ../..
          
          # Build rapidgzip
          cd rapidgzip/librapidarchive
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DWITH_ISAL=OFF
          make -j$(nproc) rapidgzip || echo "rapidgzip build failed"
          cd ../../..
          
          # Build zopfli
          make -C zopfli zopfli || echo "zopfli build failed"

      - name: Package binaries
        run: |
          mkdir -p binaries
          cp target/release/gzippy binaries/
          cp pigz/pigz pigz/unpigz binaries/
          cp isa-l/build/igzip binaries/ 2>/dev/null || echo "igzip not available"
          cp rapidgzip/librapidarchive/build/src/tools/rapidgzip binaries/ 2>/dev/null || echo "rapidgzip not available"
          cp zopfli/zopfli binaries/ 2>/dev/null || echo "zopfli not available"
          ls -la binaries/

      - uses: actions/upload-artifact@v4
        with:
          name: binaries
          path: binaries/
          retention-days: 1

  # ============================================================================
  # STAGE 2: Prepare test data and all compressed variants
  # ============================================================================
  prepare-data:
    name: Prepare ${{ matrix.data_type }} 100MB
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        data_type: [silesia, software, logs]
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup binaries
        run: chmod +x ./bin/*

      - name: Generate test data
        run: |
          SIZE_BYTES=$((100 * 1024 * 1024))
          
          if [ "${{ matrix.data_type }}" = "silesia" ]; then
            # Silesia approximation: Mixed binary and text
            python3 -c "
          import random
          random.seed(42)
          with open('/tmp/test-data.bin', 'wb') as f:
              # Mix of random bytes, repetitive text, and zeros
              for _ in range(100):
                  f.write(random.randbytes(512 * 1024)) # Random
                  f.write(b'The quick brown fox jumps over the lazy dog\n' * 10000) # Repetitive
                  f.write(b'\x00' * 64 * 1024) # Sparse
          "
          elif [ "${{ matrix.data_type }}" = "software" ]; then
            # Software: High match density (source code patterns)
            python3 -c "
          patterns = [
              b'    pub fn new() -> Self {\n        Self {\n            data: Vec::with_capacity(1024),\n            count: 0,\n        }\n    }\n',
              b'    #[inline(always)]\n    pub fn get_count(&self) -> usize {\n        self.count\n    }\n',
              b'// TODO: Implement SIMD optimization for this loop\nfor i in 0..data.len() {\n    sum += data[i] as u64;\n}\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/test-data.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = patterns[i % len(patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
          else
            # Logs: Extreme repetition (structured logs)
            python3 -c "
          log_patterns = [
              b'2026-01-20 14:30:05 INFO [com.gzippy.core] Processed block 1024 in 5.2ms\n',
              b'2026-01-20 14:30:05 DEBUG [com.gzippy.sched] Lane 0 claimed chunk at offset 0x4000\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/test-data.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = log_patterns[i % len(log_patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
          fi
          ls -lh /tmp/test-data.bin

      - name: Create all compressed variants
        run: |
          mkdir -p /tmp/compressed
          
          if [ "${{ matrix.data_type }}" = "silesia" ]; then
            # SILESIA: flate2 Best (standard dynamic blocks)
            # We'll call this 'zlib' source for the benchmark report
            gzip -9 -c /tmp/test-data.bin > /tmp/compressed/zlib-L9.gz
          elif [ "${{ matrix.data_type }}" = "software" ]; then
            # SOFTWARE: libdeflate L12 (dense LZ77 matches)
            ./bin/gzippy -9 -c /tmp/test-data.bin > /tmp/compressed/libdeflate-L12.gz
          else
            # LOGS: libdeflate L1 (fastest/simple blocks)
            ./bin/gzippy -1 -c /tmp/test-data.bin > /tmp/compressed/libdeflate-L1.gz
          fi
          
          # Also create pigz baseline for every data type
          ./bin/pigz -6 -c /tmp/test-data.bin > /tmp/compressed/pigz-L6.gz
          
          ls -lh /tmp/compressed/

      - uses: actions/upload-artifact@v4
        with:
          name: data-${{ matrix.data_type }}
          path: |
            /tmp/test-data.bin
            /tmp/compressed/
          retention-days: 1

  # ============================================================================
  # STAGE 3: Run benchmarks (compression + decompression)
  # ============================================================================
  benchmark:
    name: Bench ${{ matrix.data_type }} L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: prepare-data
    strategy:
      fail-fast: false
      matrix:
        data_type: [silesia, software, logs]
        level: [1, 6, 9]
        threads: ["1", "max"]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: data-${{ matrix.data_type }}
          path: ./data

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results
          
          # Resolve data paths
          if [ -f "./data/test-data.bin" ]; then
            echo "DATA_FILE=./data/test-data.bin" >> $GITHUB_ENV
            echo "COMPRESSED_DIR=./data/compressed" >> $GITHUB_ENV
          else
            echo "DATA_FILE=./data/tmp/test-data.bin" >> $GITHUB_ENV
            echo "COMPRESSED_DIR=./data/tmp/compressed" >> $GITHUB_ENV
          fi
          
          # Resolve thread count
          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Run unified benchmark
        run: |
          python3 scripts/run_benchmarks.py \
            --binaries ./bin \
            --data-file "$DATA_FILE" \
            --compressed-dir "$COMPRESSED_DIR" \
            --level ${{ matrix.level }} \
            --threads $THREADS \
            --size 100 \
            --data-type ${{ matrix.data_type }} \
            --output results/benchmark.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.data_type }}-100mb-l${{ matrix.level }}-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 3b: Single-Member Large File Benchmark (tests 4-phase pipeline)
  # ============================================================================
  # This specifically tests single-member gzip files (created by standard gzip),
  # which exercise the hyper-parallel 4-phase pipeline:
  #   Phase 1: Window Boot (sequential first chunk for 32KB window)
  #   Phase 2: Speculative Parallel Decode (markers for unresolved back-refs)
  #   Phase 3: Window Propagation + SIMD Marker Replacement
  #   Phase 4: Write Output
  #
  # Uses 500MB tarball to ensure parallel overhead is worth it.
  # ============================================================================
  single-member:
    name: Single-Member 500MB ${{ matrix.threads }}T
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        threads: ["1", "max"]  # 0 = max cores

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results
          
          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Generate 500MB tarball (single-member gzip)
        run: |
          echo "Creating 500MB tarball from /usr..."
          
          # Create a real tarball from /usr (realistic mixed content)
          tar cf - /usr/share /usr/lib 2>/dev/null | head -c $((500 * 1024 * 1024)) > /tmp/giant.tar
          
          # Pad to exactly 500MB if needed
          CURRENT_SIZE=$(stat -c%s /tmp/giant.tar)
          if [ "$CURRENT_SIZE" -lt $((500 * 1024 * 1024)) ]; then
            dd if=/dev/zero bs=1 count=$((500 * 1024 * 1024 - CURRENT_SIZE)) >> /tmp/giant.tar 2>/dev/null
          fi
          
          echo "Tarball size: $(stat -c%s /tmp/giant.tar) bytes"
          
          # Compress with standard gzip -6 (creates single-member file)
          # This is the key: NOT pigz or gzippy, which create multi-member
          echo "Compressing with gzip -6 (single-member)..."
          gzip -6 -c /tmp/giant.tar > /tmp/giant.tar.gz
          
          echo "Compressed size: $(stat -c%s /tmp/giant.tar.gz) bytes"
          echo "Ratio: $(echo "scale=2; $(stat -c%s /tmp/giant.tar.gz) * 100 / $(stat -c%s /tmp/giant.tar)" | bc)%"

      - name: Benchmark single-member decompression
        run: |
          python3 scripts/benchmark_single_member.py \
            --binaries ./bin \
            --compressed-file /tmp/giant.tar.gz \
            --original-file /tmp/giant.tar \
            --threads $THREADS \
            --output results/single-member.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-single-member-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 3c: Random data edge cases
  # ============================================================================
  random-data:
    name: Random Data L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        level: [1, 9]
        threads: ["1", "max"]

    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup
        run: |
          chmod +x ./bin/*
          
          # Resolve thread count
          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Test random data
        run: |
          # Generate 10MB random data
          dd if=/dev/urandom of=/tmp/random.bin bs=1M count=10 2>/dev/null
          
          # Compress
          cat /tmp/random.bin | ./bin/gzippy -${{ matrix.level }} -p$THREADS > /tmp/random.gz
          
          # Decompress with gzippy
          ./bin/gzippy -d < /tmp/random.gz > /tmp/random.out
          diff /tmp/random.bin /tmp/random.out
          echo "âœ“ gzippy roundtrip OK"
          
          # Verify gzip compatibility
          gzip -d < /tmp/random.gz > /tmp/random.gzip.out
          diff /tmp/random.bin /tmp/random.gzip.out
          echo "âœ“ gzip can decompress gzippy output"

  # ============================================================================
  # STAGE 4: Apply performance guards (threshold assertions)
  # ============================================================================
  guards:
    name: Performance Guards
    runs-on: ubuntu-latest
    needs: [benchmark, single-member, random-data]
    if: always() && needs.benchmark.result == 'success'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: results/
          merge-multiple: true

      - name: Collect system info
        run: python3 scripts/collect_system_info.py --output results/system.json

      - name: Aggregate results
        run: |
          python3 scripts/aggregate_benchmark_results.py \
            --input-dir results \
            --output-dir aggregated

      - name: Check performance guards
        id: guards
        run: |
          python3 scripts/check_guards.py \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output guards-report.json

      - name: Generate summary
        run: |
          python3 scripts/generate_summary.py \
            --system results/system.json \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output summary.md
          
          cat summary.md >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: |
            aggregated/
            results/system.json
            guards-report.json
            summary.md
          retention-days: 30

      - name: Post to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let summary;
            try {
              summary = fs.readFileSync('summary.md', 'utf8');
            } catch {
              summary = '## gzippy Performance Summary\n\nNo results available.';
            }
            
            const body = summary + '\n\n---\n*Updated: ' + new Date().toISOString() + '*';
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            const marker = '# ðŸš€ gzippy Performance Summary';
            const existing = comments.find(c => 
              c.user.type === 'Bot' && c.body && c.body.includes(marker)
            );
            
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

# =============================================================================
# Benchmarks: Compression and Decompression Performance
# =============================================================================
# Architecture:
#   Stage 1: Build all tools ONCE
#   Stage 2: Prepare test data for compression (3 content types)
#   Stage 3a: Compression benchmarks (content Ã— level Ã— threads)
#   Stage 3b: Prepare decompression archives (4 diverse structures)
#   Stage 3c: Decompression benchmarks (archive_type Ã— threads)
#   Stage 4: Guards - apply threshold assertions
#   Stage 5: Summary - aggregate and post to PR
#
# Compression tests: 3 content types Ã— 3 levels Ã— 2 thread configs = 18 jobs
# Decompression tests: 4 archive types Ã— 2 thread configs = 8 jobs
# =============================================================================
name: Benchmarks

on:
  push:
    branches: ["*"]
  pull_request:
    branches: ["*"]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  # ============================================================================
  # STAGE 1: Build all tools once, share via artifacts
  # ============================================================================
  build:
    name: Build Tools
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y build-essential cmake nasm zlib1g-dev

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}

      - name: Build all tools
        run: |
          # Build gzippy
          cargo build --release

          # Build pigz
          make -C pigz

          # Build igzip (static)
          cd isa-l
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF
          make -j$(nproc) igzip || echo "igzip build failed"
          cd ../..

          # Build rapidgzip
          cd rapidgzip/librapidarchive
          rm -rf build && mkdir -p build && cd build
          cmake .. -DCMAKE_BUILD_TYPE=Release -DWITH_ISAL=OFF
          make -j$(nproc) rapidgzip || echo "rapidgzip build failed"
          cd ../../..

          # Build zopfli
          make -C zopfli zopfli || echo "zopfli build failed"

      - name: Package binaries
        run: |
          mkdir -p binaries
          cp target/release/gzippy binaries/
          cp pigz/pigz pigz/unpigz binaries/
          cp isa-l/build/igzip binaries/ 2>/dev/null || echo "igzip not available"
          cp rapidgzip/librapidarchive/build/src/tools/rapidgzip binaries/ 2>/dev/null || echo "rapidgzip not available"
          cp zopfli/zopfli binaries/ 2>/dev/null || echo "zopfli not available"
          ls -la binaries/

      - uses: actions/upload-artifact@v4
        with:
          name: binaries
          path: binaries/
          retention-days: 1

  # ============================================================================
  # STAGE 2: Prepare compression test data (3 content types)
  # ============================================================================
  prepare-compression-data:
    name: Prepare ${{ matrix.content_type }} 100MB
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        content_type: [silesia, software, logs]
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup binaries
        run: chmod +x ./bin/*

      - name: Generate test data
        run: |
          if [ "${{ matrix.content_type }}" = "silesia" ]; then
            # Silesia approximation: Mixed binary and text
            python3 -c "
          import random
          random.seed(42)
          with open('/tmp/test-data.bin', 'wb') as f:
              for _ in range(100):
                  f.write(random.randbytes(512 * 1024))
                  f.write(b'The quick brown fox jumps over the lazy dog\n' * 10000)
                  f.write(b'\x00' * 64 * 1024)
          "
          elif [ "${{ matrix.content_type }}" = "software" ]; then
            # Software: High match density (source code patterns)
            python3 -c "
          patterns = [
              b'    pub fn new() -> Self {\n        Self {\n            data: Vec::with_capacity(1024),\n            count: 0,\n        }\n    }\n',
              b'    #[inline(always)]\n    pub fn get_count(&self) -> usize {\n        self.count\n    }\n',
              b'// TODO: Implement SIMD optimization for this loop\nfor i in 0..data.len() {\n    sum += data[i] as u64;\n}\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/test-data.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = patterns[i % len(patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
          else
            # Logs: Extreme repetition (structured logs)
            python3 -c "
          log_patterns = [
              b'2026-01-20 14:30:05 INFO [com.gzippy.core] Processed block 1024 in 5.2ms\n',
              b'2026-01-20 14:30:05 DEBUG [com.gzippy.sched] Lane 0 claimed chunk at offset 0x4000\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/test-data.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = log_patterns[i % len(log_patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
          fi
          ls -lh /tmp/test-data.bin

      - uses: actions/upload-artifact@v4
        with:
          name: compression-data-${{ matrix.content_type }}
          path: /tmp/test-data.bin
          retention-days: 1

  # ============================================================================
  # STAGE 3a: Compression benchmarks (content Ã— level Ã— threads)
  # ============================================================================
  compression-benchmark:
    name: Compress ${{ matrix.content_type }} L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: prepare-compression-data
    strategy:
      fail-fast: false
      matrix:
        content_type: [silesia, software, logs]
        level: [1, 6, 9]
        threads: ["1", "max"]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: compression-data-${{ matrix.content_type }}
          path: ./data

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Run compression benchmark
        run: |
          python3 scripts/benchmark_compression.py \
            --binaries ./bin \
            --data-file ./data/test-data.bin \
            --level ${{ matrix.level }} \
            --threads $THREADS \
            --content-type ${{ matrix.content_type }} \
            --output results/compression.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-compress-${{ matrix.content_type }}-l${{ matrix.level }}-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 3b: Prepare decompression archives (4 diverse structures)
  # ============================================================================
  # Archive diversity:
  #   1. silesia-dynamic: Mixed content, dynamic Huffman, multi-block (gzip -9)
  #   2. software-dense: Source code, dense LZ77 matches (libdeflate L12)
  #   3. logs-fast: High repetition, simpler blocks (libdeflate L1)
  #   4. tarball-single: Real /usr content, single-member 500MB (gzip -6)
  # ============================================================================
  prepare-decompression-archives:
    name: Prepare ${{ matrix.archive_type }}
    runs-on: ubuntu-latest
    needs: build
    strategy:
      matrix:
        archive_type: [silesia-dynamic, software-dense, logs-fast, tarball-single]
    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup binaries
        run: chmod +x ./bin/*

      - name: Create archive
        run: |
          mkdir -p /tmp/archive

          case "${{ matrix.archive_type }}" in
            silesia-dynamic)
              # Mixed binary/text content, compressed with gzip -9
              # Creates dynamic Huffman blocks, multi-block structure
              python3 -c "
          import random
          random.seed(42)
          with open('/tmp/original.bin', 'wb') as f:
              for _ in range(100):
                  f.write(random.randbytes(512 * 1024))
                  f.write(b'The quick brown fox jumps over the lazy dog\n' * 10000)
                  f.write(b'\x00' * 64 * 1024)
          "
              gzip -9 -c /tmp/original.bin > /tmp/archive/compressed.gz
              echo "silesia-dynamic: Dynamic Huffman, multi-block, mixed content"
              ;;

            software-dense)
              # Source code patterns, compressed with libdeflate L12
              # Creates dense LZ77 match sequences
              python3 -c "
          patterns = [
              b'    pub fn new() -> Self {\n        Self {\n            data: Vec::with_capacity(1024),\n            count: 0,\n        }\n    }\n',
              b'    #[inline(always)]\n    pub fn get_count(&self) -> usize {\n        self.count\n    }\n',
              b'// TODO: Implement SIMD optimization for this loop\nfor i in 0..data.len() {\n    sum += data[i] as u64;\n}\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/original.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = patterns[i % len(patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
              # Use gzippy at L12 (libdeflate) for dense match compression
              ./bin/gzippy --level 12 -c /tmp/original.bin > /tmp/archive/compressed.gz
              echo "software-dense: Dense LZ77 matches, source code patterns"
              ;;

            logs-fast)
              # Structured logs, compressed with libdeflate L1
              # Creates simpler block structures, high repetition
              python3 -c "
          log_patterns = [
              b'2026-01-20 14:30:05 INFO [com.gzippy.core] Processed block 1024 in 5.2ms\n',
              b'2026-01-20 14:30:05 DEBUG [com.gzippy.sched] Lane 0 claimed chunk at offset 0x4000\n',
          ]
          target = 100 * 1024 * 1024
          with open('/tmp/original.bin', 'wb') as f:
              written = 0
              i = 0
              while written < target:
                  p = log_patterns[i % len(log_patterns)]
                  f.write(p)
                  written += len(p)
                  i += 1
          "
              ./bin/gzippy -1 -c /tmp/original.bin > /tmp/archive/compressed.gz
              echo "logs-fast: Simple blocks, extreme repetition"
              ;;

            tarball-single)
              # Real /usr content as tarball, single-member gzip
              # Tests hyper-parallel 4-phase pipeline
              echo "Creating 500MB tarball from /usr..."
              tar cf - /usr/share /usr/lib 2>/dev/null | head -c $((500 * 1024 * 1024)) > /tmp/original.bin

              CURRENT_SIZE=$(stat -c%s /tmp/original.bin)
              if [ "$CURRENT_SIZE" -lt $((500 * 1024 * 1024)) ]; then
                dd if=/dev/zero bs=1 count=$((500 * 1024 * 1024 - CURRENT_SIZE)) >> /tmp/original.bin 2>/dev/null
              fi

              # Compress with standard gzip (creates single-member file)
              gzip -6 -c /tmp/original.bin > /tmp/archive/compressed.gz
              echo "tarball-single: Single-member, 500MB real content"
              ;;
          esac

          ls -lh /tmp/original.bin /tmp/archive/compressed.gz
          cp /tmp/original.bin /tmp/archive/original.bin

      - uses: actions/upload-artifact@v4
        with:
          name: decompress-archive-${{ matrix.archive_type }}
          path: /tmp/archive/
          retention-days: 1

  # ============================================================================
  # STAGE 3c: Decompression benchmarks (archive_type Ã— threads)
  # ============================================================================
  decompression-benchmark:
    name: Decompress ${{ matrix.archive_type }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: prepare-decompression-archives
    strategy:
      fail-fast: false
      matrix:
        archive_type: [silesia-dynamic, software-dense, logs-fast, tarball-single]
        threads: ["1", "max"]
    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - uses: actions/download-artifact@v4
        with:
          name: decompress-archive-${{ matrix.archive_type }}
          path: ./archive

      - name: Setup
        run: |
          chmod +x ./bin/*
          mkdir -p results

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Run decompression benchmark
        run: |
          python3 scripts/benchmark_decompression.py \
            --binaries ./bin \
            --compressed-file ./archive/compressed.gz \
            --original-file ./archive/original.bin \
            --threads $THREADS \
            --archive-type ${{ matrix.archive_type }} \
            --output results/decompression.json

      - uses: actions/upload-artifact@v4
        with:
          name: results-decompress-${{ matrix.archive_type }}-t${{ matrix.threads }}
          path: results/
          retention-days: 7

  # ============================================================================
  # STAGE 3d: Random data edge cases
  # ============================================================================
  random-data:
    name: Random Data L${{ matrix.level }} T${{ matrix.threads }}
    runs-on: ubuntu-latest
    needs: build
    strategy:
      fail-fast: false
      matrix:
        level: [1, 9]
        threads: ["1", "max"]

    steps:
      - uses: actions/download-artifact@v4
        with:
          name: binaries
          path: ./bin

      - name: Setup
        run: |
          chmod +x ./bin/*

          if [ "${{ matrix.threads }}" = "max" ]; then
            echo "THREADS=$(nproc)" >> $GITHUB_ENV
          else
            echo "THREADS=${{ matrix.threads }}" >> $GITHUB_ENV
          fi

      - name: Test random data
        run: |
          # Generate 10MB random data
          dd if=/dev/urandom of=/tmp/random.bin bs=1M count=10 2>/dev/null

          # Compress
          cat /tmp/random.bin | ./bin/gzippy -${{ matrix.level }} -p$THREADS > /tmp/random.gz

          # Decompress with gzippy
          ./bin/gzippy -d < /tmp/random.gz > /tmp/random.out
          diff /tmp/random.bin /tmp/random.out
          echo "âœ“ gzippy roundtrip OK"

          # Verify gzip compatibility
          gzip -d < /tmp/random.gz > /tmp/random.gzip.out
          diff /tmp/random.bin /tmp/random.gzip.out
          echo "âœ“ gzip can decompress gzippy output"

  # ============================================================================
  # STAGE 4: Apply performance guards (threshold assertions)
  # ============================================================================
  guards:
    name: Performance Guards
    runs-on: ubuntu-latest
    needs: [compression-benchmark, decompression-benchmark, random-data]
    if: always() && needs.compression-benchmark.result == 'success' && needs.decompression-benchmark.result == 'success'

    steps:
      - uses: actions/checkout@v4

      - uses: actions/download-artifact@v4
        with:
          pattern: results-*
          path: results/
          merge-multiple: true

      - name: Collect system info
        run: python3 scripts/collect_system_info.py --output results/system.json

      - name: Aggregate results
        run: |
          python3 scripts/aggregate_benchmark_results.py \
            --input-dir results \
            --output-dir aggregated

      - name: Check performance guards
        id: guards
        run: |
          python3 scripts/check_guards.py \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output guards-report.json

      - name: Generate summary
        run: |
          python3 scripts/generate_summary.py \
            --system results/system.json \
            --compression aggregated/compression.json \
            --decompression aggregated/decompression.json \
            --output summary.md
          
          cat summary.md >> $GITHUB_STEP_SUMMARY

      - uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: |
            aggregated/
            results/system.json
            guards-report.json
            summary.md
          retention-days: 30

      - name: Post to PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let summary;
            try {
              summary = fs.readFileSync('summary.md', 'utf8');
            } catch {
              summary = '## gzippy Performance Summary\n\nNo results available.';
            }
            
            const body = summary + '\n\n---\n*Updated: ' + new Date().toISOString() + '*';
            
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            
            const marker = '# ðŸš€ gzippy Performance Summary';
            const existing = comments.find(c => 
              c.user.type === 'Bot' && c.body && c.body.includes(marker)
            );
            
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

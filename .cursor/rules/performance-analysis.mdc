---
description: CI-driven performance workflow — cloud fleet is the source of truth
alwaysApply: true
---

# Performance Workflow: Cloud Fleet Is the Source of Truth

## Core Principle

**Never trust local benchmarks for final decisions.** The cloud fleet runs on both
x86_64 (c7i Sapphire Rapids) and arm64 (c8g Graviton4) with RAM-backed I/O,
all datasets, archive types, thread configs, and competitors. Local runs are
useful for rapid iteration but the cloud fleet is authoritative.

**Never compromise performance under any conditions.** There is no threshold below
which a regression is acceptable. Never accept a clippy/lint suggestion in a hot path
without verifying identical codegen. Use `#[allow(clippy::...)]` to suppress lints
rather than changing code that might be slower. Every nanosecond counts.

## Full Benchmark Matrix

### Decompression: 3 datasets × 3 archives × 2 threads × 2 archs = 36 scenarios
| Axis | Values |
|------|--------|
| Datasets | silesia (211MB), software (~22MB), logs (~22MB) |
| Archive types | gzip (single-member), bgzf (gzippy format), pigz (single-member!) |
| Thread configs | T1 (1 thread), Tmax (4 on cloud fleet) |
| Architectures | x86_64 (c7i.4xlarge), arm64 (c8g.4xlarge) |
| Competitors | unpigz, igzip, rapidgzip, gzip |

### Compression: 3 datasets × 3 levels × 2 threads × 2 archs = 36 scenarios
| Axis | Values |
|------|--------|
| Datasets | silesia (211MB), software (~22MB), logs (~22MB) |
| Levels | L1 (speed), L6 (balanced), L9 (ratio) |
| Thread configs | T1 (1 thread), Tmax (4 on cloud fleet) |
| Architectures | x86_64 (c7i.4xlarge), arm64 (c8g.4xlarge) |
| Competitors | pigz, igzip (L1 only), gzip |

### Total: 72 scenarios (36 decompress + 36 compress)
Plus local Mac (arm64 Apple Silicon) for all scenarios = 108 total cells.

## Benchmarking Architecture (Feb 2026)

**ONE benchmark implementation**: `gzippy-dev bench` is THE authoritative benchmark.
It runs everywhere — local, cloud fleet, CI — with identical logic.

```
gzippy-dev bench                        # decompression, all datasets
gzippy-dev bench --direction compress   # compression benchmarks (L1, L6, L9)
gzippy-dev bench --direction both       # everything
gzippy-dev bench --json                 # machine-readable
gzippy-dev bench --dataset silesia      # one dataset
gzippy-dev bench --archive bgzf         # one archive type
gzippy-dev bench --threads 1            # T1 only
gzippy-dev bench --min-trials 50 --max-trials 200 --target-cv 0.005
```

**Cloud fleet** (`gzippy-dev cloud bench`):
- 12 instances: 6 x86_64 (c7i.4xlarge) + 6 arm64 (c8g.4xlarge)
- 1 instance per (arch, dataset, direction) — fully parallel
- Also runs local Mac benchmarks simultaneously (both directions)
- Two-phase: sweep (30-100 trials, CV<1.5%) + precision re-run close races
- Strict scoring: gzippy must be >= every competitor, or it's a loss
- Results dumped to `cloud-results.json` with per-scenario scorecard
- Wall time: ~20 min total (setup + benchmarks in parallel)

**Output**: `cloud-results.json` contains structured results with scorecard:
```json
{
  "wins": N, "losses": M, "total_scenarios": N+M,
  "scorecard": [{"scenario": "...", "verdict": "WIN/LOSS", "gap_pct": ...}],
  "results": [{"platform": "...", "dataset": "...", "direction": "...", ...}]
}
```

## The Loop

```
# ... make ONE focused code change ...
cargo test --release                  # 1. Correctness
# local benchmark to sanity-check
gzippy-dev cloud bench                # 2. Authoritative cloud fleet numbers
# read cloud-results.json for structured results
```

## Current Status: ~36W / ~36L (Feb 21 2026, Cloud Fleet - partial 10/12)

### Decompression: 18 WINS / 18 LOSSES

**WINS (18):**
- **BGZF — ALL 12** (T1 + Tmax, both arches): +4% to +167% vs best
- **arm64 non-BGZF T1 — ALL 6**: +7% to +76% vs rapidgzip/unpigz
- **x86_64 non-BGZF T1 — 0** (igzip beats us on all non-BGZF)

**LOSSES (18):**
- x86_64 non-BGZF T1 vs igzip (6): igzip's AVX-512 CRC32 + Huffman gives 5-58% lead
- All non-BGZF Tmax (12): rapidgzip parallelizes single-member; gzippy is sequential
  - Note: pigz output IS single-member (not multi-member as previously assumed!)

### Compression: 18+ WINS / ~6 LOSSES (2 instances still running)

**WINS:**
- ALL T1 (all datasets, levels, both arches) except x86 L1 vs igzip
- ARM Tmax L6/L9 (all datasets)
- x86 Tmax L9 (all datasets)

**LOSSES:**
- L1 Tmax: pigz 3.5x scaling vs gzippy 1.2-1.3x scaling (block overhead)
- x86 L6 Tmax: pigz slightly faster on software (-11%)

### Key Findings from Cloud Fleet (Feb 21 2026)

**Critical discovery: pigz output is SINGLE-MEMBER gzip.**
Phase 2 (multi-member parallel decompression) was based on a wrong assumption.
All pigz-format losses are actually the same root cause as gzip-format losses:
no single-member parallel decompression. The 6 Phase 2 losses merge into Phase 4.

**Fixed: multi-member scan was doing 2x work.**
`scan_member_boundaries_exact` fully decompressed every member to find boundaries,
then `decompress_multi_member_parallel` re-decompressed them. Replaced with O(N)
magic-byte scan (`scan_member_boundaries_fast`). Fixes actual multi-member files
(like concatenated gzip streams) but doesn't affect CI benchmarks (pigz/gzip are
single-member).

**Phase 3 (ISA-L inflate for x86 T1) implemented:**
Added `isal_decompress.rs` with ISA-L decompression bindings gated behind
`#[cfg(all(feature = "isal-compression", target_arch = "x86_64"))]`. Routes
x86_64 single-member decompression through ISA-L's AVX2/AVX-512 inflate.
Expected to close 6 x86 T1 decompression losses.

**Compression L1 Tmax gap persists:**
Block size increase from 256KB→4MB helped L6/L9 scaling but L1 Tmax is still
1.2-1.3x vs pigz's 3.5x. Root cause: per-block overhead (hash table init,
header/trailer writes, CRC32) dominates at L1's fast compression speeds.

## Optimization History: What Works vs What Doesn't

### Strategies That WORK
| Approach | Result |
|---|---|
| Pipelined BGZF streaming | 2.88x scaling, +71-155% vs before |
| Per-thread reusable buffers | Stays in L2 cache |
| RAM-backed I/O (/dev/shm) | Reveals true CPU speed |
| Parallel fleet (1 per arch×dataset×direction) | Full parallelism, high precision |
| Local Mac in parallel with cloud | Free extra data point |
| JSON results dump | Easy to parse for analysis |

### Strategies That DON'T WORK
| Approach | Result |
|---|---|
| Two-pass parallel (scan + re-decode) | 32% SLOWER (scan cost too high at 4 cores) |
| Full-decompression boundary scan | 2x total work (scan decompresses, then parallel re-decompresses) |
| Speculative parallel | Regressed arm64 by 15% |
| Assuming pigz=multi-member | pigz output IS single-member; can't parallelize via member splitting |
| Simulation benchmarks | Always lie about real-world perf |
| EBS gp3 for benchmarks | 131 MB/s cap hides real differences |
| Micro-optimizations in decode | LLVM already optimizes well |
| Pre-allocated full output + serial write_all | 2x scaling, 53K page faults |

## Cloud Fleet Resilience (Feb 21 2026)

**Failure modes fixed:**
- SSH command timeout (25min bench, 15s poll) — prevents indefinite hangs
- Graceful degradation: failed instances are skipped, results from rest are collected
- SG cleanup retries with exponential backoff (10/20/40s)
- Progress counter: "3/12 complete, 0 failed" printed as instances finish
- Setup failure isolation: one failed instance doesn't abort the fleet

**Remaining concern:**
- Silesia L9 compression takes ~15-20 minutes per instance (100 trials × 3 levels × slow speed)
- This is the fleet bottleneck — consider reducing trial count for slow operations

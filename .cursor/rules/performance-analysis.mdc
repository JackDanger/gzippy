---
description: CI-driven performance workflow — CI is the source of truth, not local benchmarks
alwaysApply: true
---

# Performance Workflow: CI Is the Source of Truth

## Core Principle

**Never trust local benchmarks for final decisions.** GHA CI runs on both x86_64 (EPYC)
and arm64 (Graviton) with all datasets, archive types, thread configs, and competitors.
Local runs are useful for rapid iteration but CI is authoritative.

**Never compromise performance under any conditions.** There is no threshold below
which a regression is acceptable. Never accept a clippy/lint suggestion in a hot path
without verifying identical codegen. Use `#[allow(clippy::...)]` to suppress lints
rather than changing code that might be slower. Every nanosecond counts.

## Benchmarking Architecture (Feb 2026)

**ONE benchmark implementation**: `gzippy-dev bench` is THE authoritative benchmark.
It runs everywhere — local, cloud fleet, CI — with identical logic.

```
gzippy-dev bench                    # human output, all datasets/archives/threads
gzippy-dev bench --json             # machine-readable, same data
gzippy-dev bench --dataset silesia  # one dataset
gzippy-dev bench --archive bgzf     # one archive type
gzippy-dev bench --threads 1        # T1 only
gzippy-dev bench --min-trials 50 --max-trials 200 --target-cv 0.005  # precision
```

**Cloud fleet** (`gzippy-dev cloud bench`): launches x86 + arm64 EC2 instances,
builds gzippy-dev on them, runs `gzippy-dev bench --json` via SSH, two-phase
sweep+precision, strict scoring. Same code, same logic, deterministic numbers.

**Key cloud learnings**:
- EBS gp3 caps at ~131 MB/s; ALL data MUST be in /dev/shm (RAM) or numbers are wrong
- On-demand instances avoid spot capacity errors; cost is negligible for ~15min runs
- Two-phase: sweep at CV<2%, then precision re-run close races at CV<0.5%
- Python benchmark scripts (`scripts/benchmark_*.py`) remain for CI compatibility
  but gzippy-dev bench is the source of truth

## The Loop

```
gzippy-dev ci triage          # 1. What gaps remain?
# ... make ONE focused code change ...
gzippy-dev ci push            # 2. Push → CI → auto-triage + vs-main
gzippy-dev ci vs-main         # 3. Check anytime: did branch help or hurt?
gzippy-dev cloud bench        # 4. Low-jitter verification on dedicated hardware
```

## Current Status: ~92% Effective Win Rate (Feb 20 2026)

**Effective win rate: 94.3%** (up from 83% at start of session)
- ARCHITECTURE: 4 gaps (silesia Tmax vs rapidgzip only, 10-16%)
- SIMD: 3 gaps (L1 compression vs igzip, 4-50% but worst is CI noise)
- ACTIONABLE: 0 gaps
- NOISE: 27 gaps (properly thresholded by platform/dataset)

Key wins from latest CI run (perf/bgzf-t1-fast-path vs main):
- `silesia-bgzf Tmax x86`: +10.1% (314.8 → 346.5 MB/s, now BEATS rapidgzip by 6.6%)
- `silesia-bgzf T1 x86`: +10.0% (234.0 → 257.5 MB/s)
- `silesia-gzip T1 x86`: +4.8% (254.5 → 266.6 MB/s)
- `silesia-pigz Tmax x86`: +7.6% (239.5 → 257.6 MB/s)

## What We Learned (Feb 20 2026)

### Noise Thresholds Must Vary by Context

| Context | Threshold | Why |
|---|---|---|
| arm64 + small files | 25% | CI shows 24%+ swings between identical runs (observed: 164→125 MB/s, same code) |
| arm64 + silesia | 8% | Better but still noisy |
| x86 + small files | 5% | Moderate variance from startup overhead |
| x86 + silesia | 3% | Most stable measurement |

The old flat 2% threshold miscategorized arm64 noise as ACTIONABLE, wasting effort.

### Pigz Decompresses Sequentially (Even at Tmax)

Pigz ignores `-p` for decompression. So Tmax gaps vs pigz on single-member files
are NOT architecture gaps — both tools decompress sequentially. Only rapidgzip does
parallel single-member decompression. The triage now only classifies Tmax gaps vs
rapidgzip as ARCHITECTURE.

### pigz Output Is Single-Member

pigz 2.8 creates **single-member** gzip output from stdin (even with `-p4`). Our
`is_likely_multi_member` correctly returns false. The gaps vs rapidgzip on `silesia-pigz`
are genuine architecture gaps, not detection bugs.

### Two-Pass Parallel Does NOT Beat Sequential

Tested: scan pass (pure Rust inflate) + parallel re-decode vs libdeflate FFI sequential.
Result: **32% SLOWER** on ARM (650 MB/s vs 960 MB/s).

Root cause: scan pass does full decode work (~1x), parallel pass adds ~0.25x. Total 1.25x
work at 90% of libdeflate speed = net throughput 72% of sequential. Break-even requires
11+ threads; CI has only 4.

The ONLY viable parallel single-member approach is rapidgzip-style speculative decode
(no scan pass). This remains a major feature gap.

### stdin mmap Helps When Stdin Is a Regular File

CI benchmark pipes files via `subprocess.run(cmd, stdin=fin, stdout=fout)`. Python passes
the real fd, so gzippy can mmap stdin instead of copying 77MB into a Vec. Saves allocation
+ copy overhead.

### Raw Deflate Is Faster Than gzip_decompress_ex

For trusted BGZF blocks, call `libdeflate_deflate_decompress` directly on the raw deflate
data. Skips per-block: gzip header parsing, CRC32 validation, decompressor alloc/free.
Now used in both T1 streaming and Tmax parallel BGZF paths.

### Single-Member Sequential Path Is Already Optimal

Traced the T1/Tmax single-member decompression path: mmap → format detect → one
libdeflate call → one write → flush. No wasted work. T1 and Tmax use the identical
code path (can_parallelize=false for single-member). The 1-2% x86 noise gaps vs
rapidgzip/igzip on software/logs are pure measurement variance (55ms operations).

### BGZF deflate_start Must Handle FNAME/FCOMMENT

The `parse_bgzf_blocks` function's `deflate_start = offset + 12 + xlen` is WRONG when
gzip headers have FNAME (0x08), FCOMMENT (0x10), or FHCRC (0x02) flags set. Must parse
all optional fields to find the true deflate data offset.

## Remaining Gaps

### ACTIONABLE: 1 gap (pending CI verification)

- `silesia-gzip Tmax arm64` vs pigz: -10.2% (113 vs 125.8 MB/s). Caused by
  speculative parallel overhead (thread spawn + block search before fallback).
  Fix pushed: removed speculative parallel from production path. Expecting
  Tmax to match T1 performance (~133 MB/s), closing this gap.

Previously closed:
- `silesia-bgzf Tmax x86` vs rapidgzip: **CLOSED** (+6.6% ahead, 346.5 vs 325 MB/s)
- `silesia-gzip T1 x86` vs igzip: **CLOSED** (+5.1% ahead, 266.6 vs 253.6 MB/s)
- `software-bgzf T1 arm64` vs pigz: **NOISE** (24% swing between identical code runs)

### ARCHITECTURE (4 gaps, needs major feature)
- Silesia Tmax decompression vs rapidgzip only (10-16%)
- arm64 small-file Tmax gaps reclassified as NOISE (pigz decompresses sequentially)
- Requires rapidgzip-style pipeline (dedicated block-finder + decoder threads)
- Two-pass approach proven insufficient (scan pass overhead too high on 4 cores)

### Speculative Parallel (Feb 2026 — Removed from Production)

`speculative_parallel.rs` was wired into production for Tmax single-member files,
but CI revealed it caused a **-15% regression on arm64** (133→113 MB/s on silesia).
The block finder always falls back to sequential on complex data, but thread
spawning + block search overhead is non-trivial, especially on Graviton.

**Key lesson**: Even "fast-fail" attempts have real overhead. Thread spawn/join
plus scanning 16KB windows with 8 bit offsets per thread costs enough to cause
a measurable regression. Never wire experimental code into the hot path until
it actually succeeds on representative data.

**The code is preserved** in `src/speculative_parallel.rs` (not compiled).
Re-enable only when the block finder reliably finds boundaries on silesia.

### SIMD (3 gaps, accepted)
- L1 compression vs igzip's AVX2/AVX-512 assembly

## CI Benchmark Architecture

**Platforms**: x86_64 (EPYC 4-core), arm64 (Graviton 4-core)
**Datasets**: silesia (211MB), software (~22MB), logs (~22MB)
**Archives**: gzip, bgzf, pigz — thread configs T1 and Tmax (4)
**CI path**: Python scripts + `subprocess.run([tool, "-d"], stdin=fin, stdout=fout)`
**Cloud path**: `gzippy-dev bench --json` (stdin→file methodology, same semantics)
**Convergence**: min 10, max 40 trials, target CV <3% (sweep); 50-200 trials, CV<0.5% (precision)

## What Works vs What Doesn't for Benchmarking

### Works
- RAM-backed I/O (/dev/shm): eliminates disk bottleneck, reveals true CPU speed
- On-demand instances: no capacity errors, reliable launch
- Two-phase sweep+precision: fast sweep catches obvious wins, precision resolves close races
- Trimmed stats (drop min/max): reduces outlier impact
- Same binary for bench tool on cloud and local: no drift between implementations
- ISIZE-based buffer pre-allocation: avoids speculative remaining*4 alloc

### Doesn't Work
- EBS gp3 for benchmarks: 131 MB/s cap makes all tools look identical
- Spot instances: InsufficientInstanceCapacity errors in single-AZ setup
- Simulation benchmarks: always lie about real-world performance
- Multiple benchmark implementations: drift between Python scripts vs Rust code
- Flat noise thresholds: arm64 small files have 25% variance, silesia x86 has 3%

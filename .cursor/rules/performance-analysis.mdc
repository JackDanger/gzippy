---
description: Performance status and strategy — cloud fleet is source of truth
alwaysApply: true
---

# Performance Status: 47W / 13L (Feb 21 2026, Fleet Run 3)

## Score Breakdown

| Category | W | L | Notes |
|----------|---|---|-------|
| Decompress BGZF (all) | 12 | 0 | Dominant |
| Decompress T1 (all) | 15 | 3 | 3 losses within 1.2% (noise) |
| Decompress Tmax (non-BGZF) | 4 | 8 | No parallel single-member |
| Compress L6/L9 | 12 | 0 | Dominant |
| Compress L1 | 4 | 2 | igzip AVX-512 assembly |

## What Changed This Session

| Change | Effect |
|--------|--------|
| madvise(Sequential) on mmap stdin | arm64 L1 Tmax compress: -2.1% → +0.5% (FLIPPED) |
| igzip benchmark passes `-1` (was L2 default) | igzip compress gap: 74%→59%, 63%→42% |
| T1 ISA-L uses direct buffer (no Cursor) | L1 T1 compress: 737→1160 MB/s (+57%) |
| `gzippy-dev score` and `losses` commands | Quick scorecard from cloud-results.json |
| silesia-gzip T1 x86 | -0.5% → +0.7% (FLIPPED) |
| silesia-pigz T1 x86 | -0.8% → +0.2% (FLIPPED) |

## Loss Categories

### Noise (3 losses, <1.2%) — No action needed
x86 T1 decompress vs igzip: oscillates between WIN and LOSS.

### igzip L1 compress (2 losses, -42% to -59%)
gzippy uses isal-rs which vendors ISA-L 2.31.0. The 2.5x gap suggests
isal-sys's ISA-L may be built without NASM assembly even when NASM is
installed. Standalone igzip uses our submodule built WITH NASM.

### Tmax single-member parallel decompress (8 losses, -20% to -40%)
Two-pass parallel implemented in `parallel_single_member.rs`:
- Pass 1: scan_deflate_fast (sequential, records checkpoints + 32KB windows)
- Pass 2: parallel re-decode chunks using checkpoint windows as dictionaries
- Correctly produces byte-identical output
- **BUT**: scan pass costs ~100% of sequential decode (both fully decode all blocks).
  So total work = scan + decode/threads ≈ 1.25x sequential at 4 threads (SLOWER).
  Gated behind MIN_THREADS_FOR_PARALLEL=8, won't activate on CI's 4-core machines.

To actually close this gap, need one of:
1. Pipeline architecture (block-finder thread feeds decoder threads concurrently)
2. Scan pass that's significantly cheaper than full decode (skip output writes?)
3. Speculative block finding (proven unreliable on complex data — <0.01% success)

## Proven Strategies

| Approach | Result |
|---|---|
| mmap stdin + madvise(Sequential) for Tmax compress | +44% on x86, fixes arm64 |
| BufWriter(1MB) for stdout | Reduces syscall overhead |
| Direct ISA-L FFI for decompression | Exact parity with igzip |
| Direct buffer → ISA-L (skip Cursor double-read) | +57% for L1 T1 compress |
| BGZF-specific path at T1 | +31% |

## What Doesn't Work for Parallel Single-Member

| Approach | Why it fails |
|---|---|
| Two-pass scan+redecode | Scan costs ~100% of sequential (both fully decode) |
| Our block_finder on synthetic data | Tested wrong data, gave up too early |
| Two-pass with libdeflate in pass 2 | libdeflate has no dictionary/resume API |
| More threads (4→8→16) | Diminishing returns when scan is the bottleneck |

## How Rapidgzip Actually Solves Parallel Single-Member (KEY INSIGHT)

Studied rapidgzip source in detail. Their approach is fundamentally different
from what we tried:

1. **Partition-based guessing**: Guess block positions at regular spacing
   (e.g., every 4 MiB). Don't scan — just GUESS.
2. **LUT prefilter**: 13-15 bit lookup table eliminates ~97% of positions.
   Checks: not-final-block, dynamic Huffman type, valid code counts.
3. **Precode Kraft check**: Validates precode code lengths cheaply.
4. **Try-decode validation**: Actually attempt to decode at each candidate.
   Invalid positions throw exceptions — this is EXPECTED and CHEAP.
5. **Limited search**: Only search 512 KiB per partition on failure. Bounded cost.
6. **Markers for cross-boundary refs**: 16-bit markers replace back-references
   into unknown data. Resolved when the preceding chunk's window is available.
7. **Sequential orchestrator**: Chunks are post-processed in order (for window
   propagation), but decode+replacement run in parallel thread pool.
8. **Feedback**: Confirmed block offsets feed back into the block finder.

**Why our approach failed**: We did a FULL SEQUENTIAL SCAN to find boundaries.
This costs as much as a full decode. Rapidgzip GUESSES positions and validates
cheaply, so the "finding" cost is negligible compared to actual decode work.

**What we already have**: `marker_decode.rs` has working marker-based decode.
`pipeline_tests.rs` proved oracle pipeline produces correct output. We just
need a proper partition-guess-and-validate block finder + pipeline orchestrator.

**False positives are fine**: Rapidgzip tracks `falsePositiveCount` as a stat.
Invalid positions are caught by decode exceptions and cost only a few attempts.
The 512 KiB search bound caps the worst case.

## Pipeline Test Harness (`src/pipeline_tests.rs`)

Layered oracle-verified test harness for building rapidgzip-style parallel
single-member decompression. Uses `scan_deflate_fast` as ground truth.

**Key findings from building the harness:**
- `Bits.bitsleft` has garbage in high bits (libdeflate wrapping_sub pattern).
  Use `(cp.bitsleft as u8) as usize` to get real bit count from checkpoints.
- `MarkerDecoder` correctly decodes from any checkpoint when given the right
  bit position and window — 13/13 checkpoints verified on synthetic, 665 on silesia.
- Block finder has 16% precision / 67% recall on synthetic data (only finds
  dynamic Huffman blocks, not stored/fixed).
- Oracle pipeline (exact positions + windows) produces byte-identical output.
  This proves the pipeline machinery (threading, ordering, marker resolution) works.
- Spacing/adversarial pipelines degrade gracefully when block finder fails.

## Oracle Test Harnesses (Feb 2026)

Four test harnesses verify components independently with ground-truth oracles.
All 278 tests pass (0 failures).

### Pipeline Tests (`src/pipeline_tests.rs`) — 16 tests
- `DeflateOracle` from `scan_deflate_fast` + sequential decode
- Layer 1: MarkerDecoder with known boundaries (13/13 checkpoints pass)
- Layer 2: Block finder precision (16%) / recall (67%) diagnostics
- Layer 3: Chunk decoder (search + decode composition)
- Layer 4: Pipeline with oracle/spacing/adversarial block sources
- **Key finding**: `Bits.bitsleft` has garbage in high bits (libdeflate pattern).
  Extract with `(cp.bitsleft as u8) as usize`.

### Routing Tests (`src/routing_tests.rs`) — 8 tests
- Creates BGZF, multi-member, single-member gzip from same input
- Verifies format detection (`has_bgzf_markers`, `is_likely_multi_member`)
- Each path (BGZF parallel, multi-member parallel/sequential, single-member
  libdeflate) produces byte-identical output
- Thread independence: BGZF T1-T8 all produce identical output

### Inflate Oracle (`src/inflate_oracle_tests.rs`) — 7 tests
- `consume_first` vs `libdeflate` correctness (byte-identical on silesia)
- Per-pattern performance breakdown (ARM M3 numbers):

  | Pattern | consume_first | libdeflate | Ratio |
  |---|---|---|---|
  | literals | 21,257 MB/s | 22,325 MB/s | 95.2% |
  | rle | 51,064 MB/s | 33,200 MB/s | **153.8%** (WIN) |
  | short_matches | 35,899 MB/s | 35,246 MB/s | **101.9%** (WIN) |
  | mixed | 5,877 MB/s | 8,039 MB/s | 73.1% (GAP) |
  | silesia (overall) | 1,225 MB/s | 1,366 MB/s | 89.7% |

- **Key finding**: The gap is entirely in mixed literal+match patterns.
  Literals, RLE, and short matches are at parity or faster.

### Compression Oracle (`src/compress_oracle_tests.rs`) — 7 tests
- Roundtrip correctness for all paths (libdeflate, parallel, pipelined)
- Ratio comparison across 9 path/level combinations
- Thread independence (parallel: <2% spread, pipelined: <1% spread)
- **PipelinedGzEncoder T1 deadlock**: FIXED. Root cause was compress_sequential's
  inner loop never advancing `block_data` (total_in captured AFTER compress call).
  Fix: capture `before_in` BEFORE the compress call. All thread counts now work.

## Session Findings (Feb 2026)

### decode loop cleanup
- Removed ~120 lines of `#[cfg(feature = "debug_decode")]` dump code from
  the hot path in `consume_first_decode.rs`. These compiled to no-ops but
  cluttered the source and made the fastloop hard to read.

### Pure Rust inflate performance (ARM M3)
- **94.5%** of libdeflate on silesia (1275 vs 1348 MB/s)
- **133%** of libdeflate on literals, **158%** on RLE, **414%** on short_matches
- Gap is in mixed literal+match patterns (77.9% on synthetic)
- Per-block timing: 2637 dynamic blocks, average 1134 MB/s, slowest 100 MB/s
- Remaining 5.5% gap needs inline assembly or LLVM-level work

### Parallel single-member: why it doesn't help at 4 cores
Tested both two-pass and pipelined approaches:
- Two-pass: 508 MB/s (0.71x of sequential consume_first)
- Pipelined (channels): 434 MB/s (0.61x — channel overhead)
- Sequential libdeflate: 1348 MB/s
Root cause: scan pass fully decodes the stream (same cost as sequential).
Total work = scan + decode/threads ≈ 2x at 4 threads. Can't overcome 2x overhead.
Pipelined is worse because rx.into_iter() blocks until scanner finishes anyway.
Gated behind MIN_THREADS_FOR_PARALLEL=8.

## Commands

```bash
gzippy-dev score       # Current scorecard
gzippy-dev losses      # Losses grouped by root cause
source .env && gzippy-dev cloud bench  # Run fleet (~$0.50, ~20 min)
```

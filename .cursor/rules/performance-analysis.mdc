---
description: CI-driven performance workflow — CI is the source of truth, not local benchmarks
alwaysApply: true
---

# Performance Workflow: CI Is the Source of Truth

## Core Principle

**Never trust local benchmarks for final decisions.** GHA CI runs on both x86_64 (EPYC)
and arm64 (Graviton) with all datasets, archive types, thread configs, and competitors.
Local runs are useful for rapid iteration but CI is authoritative.

**Never compromise performance under any conditions.** There is no threshold below
which a regression is acceptable. Never accept a clippy/lint suggestion in a hot path
without verifying identical codegen. Use `#[allow(clippy::...)]` to suppress lints
rather than changing code that might be slower. Every nanosecond counts.

## Benchmarking Architecture (Feb 2026)

**ONE benchmark implementation**: `gzippy-dev bench` is THE authoritative benchmark.
It runs everywhere — local, cloud fleet, CI — with identical logic.

```
gzippy-dev bench                    # human output, all datasets/archives/threads
gzippy-dev bench --json             # machine-readable, same data
gzippy-dev bench --dataset silesia  # one dataset
gzippy-dev bench --archive bgzf     # one archive type
gzippy-dev bench --threads 1        # T1 only
gzippy-dev bench --min-trials 50 --max-trials 200 --target-cv 0.005  # precision
```

**Cloud fleet** (`gzippy-dev cloud bench`): launches x86 + arm64 EC2 instances,
builds gzippy-dev on them, runs `gzippy-dev bench --json` via SSH, two-phase
sweep+precision, strict scoring. Same code, same logic, deterministic numbers.

**Key cloud learnings**:
- EBS gp3 caps at ~131 MB/s; ALL data MUST be in /dev/shm (RAM) or numbers are wrong
- On-demand instances avoid spot capacity errors; cost is negligible for ~15min runs
- Two-phase: sweep at CV<2%, then precision re-run close races at CV<0.5%
- Python benchmark scripts (`scripts/benchmark_*.py`) remain for CI compatibility
  but gzippy-dev bench is the source of truth

## The Loop

```
gzippy-dev ci triage          # 1. What gaps remain?
# ... make ONE focused code change ...
gzippy-dev ci push            # 2. Push → CI → auto-triage + vs-main
gzippy-dev ci vs-main         # 3. Check anytime: did branch help or hurt?
gzippy-dev cloud bench        # 4. Low-jitter verification on dedicated hardware
```

## Current Status: Cloud Fleet Reality Check (Feb 20 2026)

**CI win rate: ~92%** (noisy, inflated by I/O overhead hiding real gaps)
**Cloud win rate: 13/36 = 36%** (clean, RAM-backed, CPU-limited truth)

The cloud fleet revealed gaps that CI's I/O noise was hiding. Three categories:

### 1. BGZF (gzippy's format) — DOMINANT
| Scenario | gzippy | Best comp | Gap |
|---|---|---|---|
| silesia-bgzf T1 arm64 | 651 | 285 unpigz | **+128%** |
| silesia-bgzf T1 x86 | 668 | 567 igzip | **+18%** |
| logs-bgzf T1 x86 | 1972 | 1813 igzip | **+9%** |
| silesia-bgzf Tmax arm64 | 1265 | 962 rapidgzip | **+31%** |

### 2. Non-BGZF T1 arm64 — WINS (no igzip competition)
gzippy wins all 6 scenarios (+4% to +83%) because igzip has no ARM build
and unpigz/rapidgzip are slower at T1.

### 3. Non-BGZF x86 T1 — igzip AVX-512 advantage (REAL)
| Scenario | gzippy | igzip | Gap |
|---|---|---|---|
| software-gzip T1 x86 | 1190 | 2468 | **-52%** |
| logs-gzip T1 x86 | 1074 | 1841 | **-42%** |
| silesia-gzip T1 x86 | 534 | 577 | **-8%** |

igzip's AVX-512 Huffman decode + CRC32 gives 2x throughput on Ice Lake.
CI hid this because both tools were ~equally bottlenecked by disk I/O.

### 4. Tmax single-member — rapidgzip parallel advantage (ARCHITECTURE)
rapidgzip parallelizes single-member gzip files. gzippy cannot.
Gaps: -10% to -48%. Requires dedicated block-finder + decoder threads.

## What We Learned (Feb 20 2026)

### Noise Thresholds Must Vary by Context

| Context | Threshold | Why |
|---|---|---|
| arm64 + small files | 25% | CI shows 24%+ swings between identical runs (observed: 164→125 MB/s, same code) |
| arm64 + silesia | 8% | Better but still noisy |
| x86 + small files | 5% | Moderate variance from startup overhead |
| x86 + silesia | 3% | Most stable measurement |

The old flat 2% threshold miscategorized arm64 noise as ACTIONABLE, wasting effort.

### Pigz Decompresses Sequentially (Even at Tmax)

Pigz ignores `-p` for decompression. So Tmax gaps vs pigz on single-member files
are NOT architecture gaps — both tools decompress sequentially. Only rapidgzip does
parallel single-member decompression. The triage now only classifies Tmax gaps vs
rapidgzip as ARCHITECTURE.

### pigz Output Is Single-Member

pigz 2.8 creates **single-member** gzip output from stdin (even with `-p4`). Our
`is_likely_multi_member` correctly returns false. The gaps vs rapidgzip on `silesia-pigz`
are genuine architecture gaps, not detection bugs.

### Two-Pass Parallel Does NOT Beat Sequential

Tested: scan pass (pure Rust inflate) + parallel re-decode vs libdeflate FFI sequential.
Result: **32% SLOWER** on ARM (650 MB/s vs 960 MB/s).

Root cause: scan pass does full decode work (~1x), parallel pass adds ~0.25x. Total 1.25x
work at 90% of libdeflate speed = net throughput 72% of sequential. Break-even requires
11+ threads; CI has only 4.

The ONLY viable parallel single-member approach is rapidgzip-style speculative decode
(no scan pass). This remains a major feature gap.

### stdin mmap Helps When Stdin Is a Regular File

CI benchmark pipes files via `subprocess.run(cmd, stdin=fin, stdout=fout)`. Python passes
the real fd, so gzippy can mmap stdin instead of copying 77MB into a Vec. Saves allocation
+ copy overhead.

### Raw Deflate Is Faster Than gzip_decompress_ex

For trusted BGZF blocks, call `libdeflate_deflate_decompress` directly on the raw deflate
data. Skips per-block: gzip header parsing, CRC32 validation, decompressor alloc/free.
Now used in both T1 streaming and Tmax parallel BGZF paths.

### Single-Member Sequential Path Is Already Optimal

Traced the T1/Tmax single-member decompression path: mmap → format detect → one
libdeflate call → one write → flush. No wasted work. T1 and Tmax use the identical
code path (can_parallelize=false for single-member). The 1-2% x86 noise gaps vs
rapidgzip/igzip on software/logs are pure measurement variance (55ms operations).

### BGZF deflate_start Must Handle FNAME/FCOMMENT

The `parse_bgzf_blocks` function's `deflate_start = offset + 12 + xlen` is WRONG when
gzip headers have FNAME (0x08), FCOMMENT (0x10), or FHCRC (0x02) flags set. Must parse
all optional fields to find the true deflate data offset.

## Remaining Gaps

### Decompression Gap Priorities

**HIGHEST IMPACT: x86 igzip AVX-512 gap (6 scenarios, -8% to -52%)**
- Root cause: igzip uses AVX-512 vpclmulqdq for CRC32 + SIMD Huffman decode
- gzippy uses libdeflate which is fast but doesn't exploit AVX-512 as aggressively
- The gap is largest on repetitive data (software -52%, logs -42%)
- On silesia (complex data) the gap is only -8%, close to noise
- Fix: either implement AVX-512 Huffman decode or skip CRC32 when not needed

**ARCHITECTURE: Tmax single-member parallel (12 scenarios, -10% to -48%)**
- rapidgzip parallelizes single-member gzip via speculative decode
- gzippy falls back to sequential for single-member files
- Two-pass approach proven insufficient (scan pass cost too high at 4 cores)
- Speculative parallel was tried but regressed arm64 by 15% (removed)
- Requires dedicated block-finder + decoder thread architecture

**BGZF Tmax on x86: rapidgzip edge (-6% to -16%)**
- Even on BGZF (multi-member), rapidgzip slightly edges gzippy at Tmax=4 on x86
- Possible cause: thread pool overhead in our parallel path
- This is the most actionable gap — our BGZF parser should win here

### What CI Was Hiding

The cloud fleet revealed that CI's ~92% win rate was inflated by I/O noise.
On fast, CPU-limited hardware:
- igzip's AVX-512 advantage is 2x on repetitive data (hidden by disk I/O on CI)
- gzippy's absolute throughput is excellent (500-2400 MB/s depending on data)
- The BGZF advantage is massive (80-128% at T1) — this is gzippy's moat

## CI Benchmark Architecture

**Platforms**: x86_64 (EPYC 4-core), arm64 (Graviton 4-core)
**Datasets**: silesia (211MB), software (~22MB), logs (~22MB)
**Archives**: gzip, bgzf, pigz — thread configs T1 and Tmax (4)
**CI path**: Python scripts + `subprocess.run([tool, "-d"], stdin=fin, stdout=fout)`
**Cloud path**: `gzippy-dev bench --json` (stdin→file methodology, same semantics)
**Convergence**: min 10, max 40 trials, target CV <3% (sweep); 50-200 trials, CV<0.5% (precision)

## What Works vs What Doesn't for Benchmarking

### Works
- RAM-backed I/O (/dev/shm): eliminates disk bottleneck, reveals true CPU speed
- On-demand instances: no capacity errors, reliable launch
- Two-phase sweep+precision: fast sweep catches obvious wins, precision resolves close races
- Trimmed stats (drop min/max): reduces outlier impact
- Same binary for bench tool on cloud and local: no drift between implementations
- ISIZE-based buffer pre-allocation: avoids speculative remaining*4 alloc

### Doesn't Work
- EBS gp3 for benchmarks: 131 MB/s cap makes all tools look identical
- Spot instances: InsufficientInstanceCapacity errors in single-AZ setup
- Simulation benchmarks: always lie about real-world performance
- Multiple benchmark implementations: drift between Python scripts vs Rust code
- Flat noise thresholds: arm64 small files have 25% variance, silesia x86 has 3%

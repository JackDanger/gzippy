---
description: CI-driven performance workflow — CI is the source of truth, not local benchmarks
alwaysApply: true
---

# Performance Workflow: Cloud Fleet Is the Source of Truth

## Core Principle

**Never trust local benchmarks for final decisions.** The cloud fleet runs on both
x86_64 (c7i Sapphire Rapids) and arm64 (c8g Graviton4) with RAM-backed I/O,
all datasets, archive types, thread configs, and competitors. Local runs are
useful for rapid iteration but the cloud fleet is authoritative.

**Never compromise performance under any conditions.** There is no threshold below
which a regression is acceptable. Never accept a clippy/lint suggestion in a hot path
without verifying identical codegen. Use `#[allow(clippy::...)]` to suppress lints
rather than changing code that might be slower. Every nanosecond counts.

## Benchmarking Architecture (Feb 2026)

**ONE benchmark implementation**: `gzippy-dev bench` is THE authoritative benchmark.
It runs everywhere — local, cloud fleet, CI — with identical logic.

```
gzippy-dev bench                    # human output, all datasets/archives/threads
gzippy-dev bench --json             # machine-readable, same data
gzippy-dev bench --dataset silesia  # one dataset
gzippy-dev bench --archive bgzf     # one archive type
gzippy-dev bench --threads 1        # T1 only
gzippy-dev bench --min-trials 50 --max-trials 200 --target-cv 0.005  # precision
```

**Cloud fleet** (`gzippy-dev cloud bench`):
- 6 instances: 3 x86_64 (c7i.4xlarge) + 3 arm64 (c8g.4xlarge)
- 1 instance per (arch, dataset) — fully parallel
- Also runs local Mac benchmarks simultaneously
- Two-phase: sweep (30-100 trials, CV<1.5%) + precision re-run close races
- Strict scoring: gzippy must be >= every competitor, or it's a loss
- Wall time: ~15 min total (setup + benchmarks in parallel)

**Key learnings**:
- EBS gp3 caps at ~131 MB/s; ALL data MUST be in /dev/shm (RAM)
- On-demand instances avoid spot capacity errors
- `TMPDIR=/dev/shm` for decompressed output too (not just compressed input)
- Python benchmark scripts remain for CI but gzippy-dev bench is truth

## The Loop

```
# ... make ONE focused code change ...
cargo test --release                  # 1. Correctness
# local benchmark to sanity-check
gzippy-dev cloud bench                # 2. Authoritative cloud fleet numbers
```

## Current Status: 18/36 Wins (Feb 20 2026, post-BGZF pipeline fix)

**Previous: 13/36 (36%).** The BGZF Tmax pipeline fix added 5 wins.

### What Changed: BGZF Tmax Pipeline

**Root cause of old 2x scaling**: `decompress_bgzf_parallel_to_vec` allocated
`vec![0u8; 211MB]` (53K page faults) then did serial `writer.write_all(211MB)`.

**Fix**: Pipelined streaming — N decoder threads decompress into per-thread
reusable buffers, send completed blocks through a bounded channel, writer thread
writes blocks in order as they arrive. No large allocation, no serial copy.

| Metric | Before | After |
|---|---|---|
| T4 scaling | 2.0x | 2.88x |
| silesia-bgzf Tmax x86 | 1329 (LOSS -16%) | 2632 (WIN +71%) |
| silesia-bgzf Tmax arm64 | 1265 (WIN +31%) | 3069 (WIN +155%) |
| logs-bgzf Tmax x86 | 1973 (LOSS -15%) | 3080 (WIN +38%) |

### BGZF — ALL 12 WINS (T1 + Tmax, both arches)
gzippy dominates its own format: +8% to +155% vs best competitor.

### 18 Remaining Losses

**Category 1: x86 T1 vs igzip (6 losses)**
| Scenario | gzippy | igzip | Gap |
|---|---|---|---|
| software-gzip T1 x86 | 1245 | 2769 | -55% |
| software-pigz T1 x86 | 1214 | 2544 | -52% |
| logs-gzip T1 x86 | 1050 | 1937 | -46% |
| logs-pigz T1 x86 | 1025 | 1771 | -42% |
| silesia-gzip T1 x86 | 524 | 585 | -10% |
| silesia-pigz T1 x86 | 518 | 539 | -4% |

igzip's AVX-512 Huffman decode + vpclmulqdq CRC32 gives 2x on repetitive data.

**Category 2: All non-BGZF Tmax (12 losses)**
rapidgzip parallelizes single-member gzip. gzippy falls back to sequential.
Gaps: -11% to -45%. Requires dedicated block-finder + decoder architecture.

## Optimization History: What Works vs What Doesn't

### BGZF Parallel
| Approach | Result |
|---|---|
| Pre-allocated full output + serial write_all | **BAD**: 2x scaling, 53K page faults |
| Pipelined streaming (channel + ordered writer) | **GOOD**: 2.88x scaling, +71-155% |
| Per-thread reusable buffers | **GOOD**: stays in L2 cache |
| Bounded sync_channel (2*threads+2 capacity) | **GOOD**: backpressure without stalling |

### Single-member Parallel (tried, not yet working)
| Approach | Result |
|---|---|
| Two-pass (scan + re-decode) | **32% SLOWER** on ARM (scan cost too high at 4 cores) |
| Speculative parallel | **Regressed** arm64 by 15% |
| Need: rapidgzip-style pipeline (block-finder + decoders) | **NOT YET IMPLEMENTED** |

### General
| Approach | Result |
|---|---|
| RAM-backed I/O (/dev/shm) | **ESSENTIAL**: reveals true CPU speed |
| 6-instance fleet (1 per arch×dataset) | **GOOD**: full parallelism, higher precision |
| Local Mac in parallel with cloud | **GOOD**: free extra data point |
| Simulation benchmarks | **BAD**: always lie about real-world perf |
| EBS gp3 for benchmarks | **BAD**: 131 MB/s cap hides real differences |

## CI Benchmark Architecture

**Cloud fleet**: c7i.4xlarge (Sapphire Rapids) + c8g.4xlarge (Graviton4)
**Datasets**: silesia (211MB/503MB decompressed), software (~22MB), logs (~22MB)
**Archives**: gzip, bgzf, pigz — thread configs T1 and Tmax=4
**Sweep**: 30-100 trials, CV<1.5%
**Precision**: 50-200 trials, CV<0.5% (close races <3%)
